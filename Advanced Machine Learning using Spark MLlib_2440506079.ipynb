{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c8f9b8-d155-4750-a26b-4731082f74c7",
   "metadata": {},
   "source": [
    "# Hands-On Pertemuan 14: Advanced Machine Learning using Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62706b6-109c-402b-b1b8-e4d400373d47",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "- Understand and implement advanced machine learning tasks using Spark MLlib.\n",
    "- Build and evaluate models using real-world datasets.\n",
    "- Explore techniques like feature engineering and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e1e12-3d2d-4d93-9916-19a45347493f",
   "metadata": {},
   "source": [
    "## Introduction to Spark MLlib\n",
    "Spark MLlib is a scalable library for machine learning that integrates seamlessly with the Spark ecosystem. It supports a wide range of tasks, including regression, classification, clustering, and collaborative filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "901b3605-15c6-4d79-ac08-562b7b611378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/22 08:22:42 WARN Instrumentation: [1a46a8ba] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear Regression with Spark MLlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
    "\n",
    "# Load sample data\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare data for modeling\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Print model coefficients\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62caa563-9abf-4784-b434-674bac769602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.262057924868083,4.0873522650485175]\n",
      "Intercept: 11.568912722344368\n"
     ]
    }
   ],
   "source": [
    "# Practice: Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName('MLlib Example2').getOrCreate()\n",
    "\n",
    "# Example dataset\n",
    "data = [(1, [2.0, 3.0], 0), (2, [1.0, 5.0], 1), (3, [2.5, 4.5], 1), (4, [3.0, 6.0], 0)]\n",
    "columns = ['ID', 'Features', 'Label']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Instead of using 'Features' directly, we need to access the elements within the array\n",
    "# Create new columns for 'Features[0]' and 'Features[1]' using Spark functions\n",
    "df = df.withColumn('Feature0', col('Features').getItem(0)) \\\n",
    "       .withColumn('Feature1', col('Features').getItem(1))\n",
    "\n",
    "# Now use VectorAssembler with the new columns\n",
    "assembler = VectorAssembler(inputCols=['Feature0', 'Feature1'], outputCol='FeaturesVector')\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Train logistic regression model using the 'FeaturesVector' column\n",
    "lr = LogisticRegression(featuresCol='FeaturesVector', labelCol='Label')\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Display coefficients and summary\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76825874-3992-4ce3-a894-ed5dc4faa776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([12.5]), array([3.])]\n"
     ]
    }
   ],
   "source": [
    "# Practice: KMeans Clustering\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Example dataset\n",
    "data = [(1, [1.0, 1.0]), (2, [5.0, 5.0]), (3, [10.0, 10.0]), (4, [15.0, 15.0])]\n",
    "columns = ['ID', 'Features']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn('Feature1', col('Features').getItem(0))\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['Feature1'], outputCol='Features_vec')\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Train KMeans clustering model\n",
    "kmeans = KMeans(featuresCol='Features_vec', k=2)\n",
    "model = kmeans.fit(df)\n",
    "\n",
    "# Show cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(f'Cluster Centers: {centers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb52db9-c7af-41fe-8c7c-ee8a24be3823",
   "metadata": {},
   "source": [
    "## Homework\n",
    "- Load a real-world dataset into Spark and prepare it for machine learning tasks.\n",
    "- Build a classification model using Spark MLlib and evaluate its performance.\n",
    "- Explore hyperparameter tuning using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16f46143-1fb1-49ba-9868-f2b1755aa683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|    species|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 1.00\n",
      "+-----------------+-----+----------+\n",
      "|         features|label|prediction|\n",
      "+-----------------+-----+----------+\n",
      "|[4.6,3.1,1.5,0.2]|  0.0|       0.0|\n",
      "|[4.7,3.2,1.6,0.2]|  0.0|       0.0|\n",
      "|[4.8,3.1,1.6,0.2]|  0.0|       0.0|\n",
      "|[4.9,3.1,1.5,0.1]|  0.0|       0.0|\n",
      "|[5.1,3.3,1.7,0.5]|  0.0|       0.0|\n",
      "|[5.1,3.8,1.5,0.3]|  0.0|       0.0|\n",
      "|[5.4,3.7,1.5,0.2]|  0.0|       0.0|\n",
      "|[5.7,4.4,1.5,0.4]|  0.0|       0.0|\n",
      "|[4.4,3.0,1.3,0.2]|  0.0|       0.0|\n",
      "|[4.9,2.4,3.3,1.0]|  1.0|       1.0|\n",
      "|[4.9,3.1,1.5,0.1]|  0.0|       0.0|\n",
      "|[5.2,2.7,3.9,1.4]|  1.0|       1.0|\n",
      "|[5.6,2.9,3.6,1.3]|  1.0|       1.0|\n",
      "|[5.8,2.7,4.1,1.0]|  1.0|       1.0|\n",
      "|[6.5,2.8,4.6,1.5]|  1.0|       1.0|\n",
      "|[6.9,3.1,4.9,1.5]|  1.0|       1.0|\n",
      "|[5.0,2.3,3.3,1.0]|  1.0|       1.0|\n",
      "|[5.5,2.6,4.4,1.2]|  1.0|       1.0|\n",
      "|[5.6,3.0,4.1,1.3]|  1.0|       1.0|\n",
      "|[5.7,2.6,3.5,1.0]|  1.0|       1.0|\n",
      "+-----------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "Best Model Parameters:\n",
      "  - regParam: 0.01\n",
      "  - elasticNetParam: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('MLlib Homework').getOrCreate()\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(url)\n",
    "iris_data = pd.read_csv(StringIO(response.text), header=None, names=columns)\n",
    "\n",
    "# Create a Spark DataFrame from the pandas DataFrame\n",
    "data = spark.createDataFrame(iris_data)\n",
    "\n",
    "# Inspect the data\n",
    "data.show(5)\n",
    "data.printSchema()\n",
    "\n",
    "# Data preparation\n",
    "# Encode categorical target column (species) to numerical\n",
    "indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
    "data = indexer.fit(data).transform(data)\n",
    "\n",
    "# Assemble feature columns into a single vector\n",
    "feature_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Build a classification model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Hyperparameter tuning with Cross-Validation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Perform cross-validation\n",
    "crossval = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "predictions = cv_model.transform(test_data)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Show results\n",
    "print(f\"Test Set Accuracy: {accuracy:.2f}\")\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "# Best model hyperparameters\n",
    "best_model = cv_model.bestModel\n",
    "print(\"Best Model Parameters:\")\n",
    "print(f\"  - regParam: {best_model._java_obj.getRegParam()}\")\n",
    "print(f\"  - elasticNetParam: {best_model._java_obj.getElasticNetParam()}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
